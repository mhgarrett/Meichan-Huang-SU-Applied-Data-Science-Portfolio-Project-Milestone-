## Project Description
In IST707 Fall 2023 held by Dr. Jeremy Bolton, I expanded my understanding of how to extract meaningful knowledge from vast datasets. From the course work, I learned a spectrum of data mining methods, gained a solid theoretical foundation of machine learning algorithms, including data preparation, concept description, association rule mining, classification, clustering, evaluation, and analysis, were comprehensive. They offered me a holistic view of the data mining landscape, enabling me to solve real-world problems with practical applications. 
For the final project, I chose to work independently with a large Kaggle dataset with 1,296,675 simulated credit card transactions, both legitimate and fraud, encompasses transactions from 1,000 customers across a network of 800 merchants and employed different classification algorithms to predict fraud cases. Since it is a binary classification problem, I have employed three popular algorithms in literature to solve this problem to identify the best performed algorithm. Specifically, traditional statistical method like logistic regression is known for their straightforward interpretability. Also, to tackle the increasing complexity of fraud patterns, I have also explored more advanced techniques. Specifically, I've utilized decision trees for their structured decision-making process and random forests to improve prediction accuracy. For this project, R studio was leveraged to conduct the data preprocessing and analysis and deploy subsequent machine learning algorithms.    
One of the paramount challenges encountered in this project was data preparation, a task that provided significant learning opportunities. The dataset mirrored the real-world imbalance between fraud and non-fraud classes. To address this, two data conditions were scrutinized: the original dataset with its inherent class imbalances and a version processed using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE, an approach that generates synthetic instances instead of merely duplicating the minority class, was instrumental in forging a more balanced dataset. This technique functions by creating synthetic examples through interpolation between a minority class observation and its nearest neighbors, thereby achieving a more equitable distribution of non-fraudulent and fraudulent cases for training purposes. Utilizing the SMOTE function led to a recalibrated dataset comprising 30,024 non-fraud and 22,518 fraud cases, effectively poised for algorithmic analysis. These two conditions were tested subsequently with all three algorisms. 
To further optimize the performance of machine learning models in the face of numerous categorical variables and substantial variance across numerical data scales, a rigorous standardization process was undertaken. This involved normalizing the numeric data ranges and converting categorical data into a uniform format. A pivotal data transformation strategy I utilized was target encoding. This method proved invaluable for managing the high dimensionality of certain categorical variables by assigning new values to each category. These values were derived based on the category's correlation with the target variable, quantified as the proportion of the target outcome's occurrences within the category's total count. Target encoding is especially beneficial for tree-based algorithms such as decision trees and random forests, as it effectively addresses the "curse of dimensionality" that plagues one-hot encoding techniques.
Furthermore, to normalize numerical data with wide-ranging values, logarithmic transformation was applied. For example, the 'amount' attribute exhibited an extensive range, prompting the use of logarithmic scaling. This transformation is adept at compressing the spread of larger values while proportionately expanding the gaps between smaller figures, thus ensuring a more uniform data distribution conducive to model training and analysis.

<img width="213" alt="image" src="https://github.com/mhgarrett/Meichan-Huang-SU-Applied-Data-Science-Portfolio-Project-Milestone-/assets/94016314/2130cd8f-0f51-4944-81c7-e76cd13f7557">
Figure 1. Log Transformation of Amount Attribute 

Additionally, due to the high dimensionality of the dataset amongst the attributes, Principal Component Analysis (PCA) were carried out, however, this step did not successfully reduce the dimensionality, as the formed clusters did not significantly reduce the number of attributes within the dataset. 
To effectively test the different data manipulation and tuning of the model, six conditions were set up and tested, Imbalanced v.s. SMOTE data using logistic regression, imbalanced and SMOTE data using Decision Tree, and imbalanced and SMOTE data using Random Forest.  Another challenge posed in this project was, considering the dataset's characteristics, standard metrics such as accuracy may not be sufficient for model evaluation. Therefore, alternative metrics, including precision (also known as specificity), recall (also known as sensitivity), F1 scores, and the Area Under the Receiver Operating Characteristic Curve (AUC), were employed to provide a more comprehensive assessment of model performance. 

<img width="311" alt="image" src="https://github.com/mhgarrett/Meichan-Huang-SU-Applied-Data-Science-Portfolio-Project-Milestone-/assets/94016314/d3bb0f3e-7c7c-4397-be11-61b50c23d1a4">


The results showed that, comparing all six models, the Random Forest model using SMOTE data stands out as the most effective in fraud detection among the three tested algorithms, with a good balance of accuracy, recall, and a reasonable rate of false positives. Also, several attributes have shown to have strong indication to fraudulent activities. a strong indication to fraud, credit card fraud tends to concentrate in certain transaction categories such as shopping (both at physical locations and online) and grocery purchases. Patterns in fraudulent activities also vary by age, with spikes among those aged 35 and 50-55, and timing, with a higher incidence in early months of the year, particularly in May, and during weekends and late nights. Moreover, states with large populations and economic significance, like New York and California, report higher levels of fraud, possibly reflecting their dense and affluent environments. However, gender is not a strong indicator of fraud in this dataset.  

## Reflection and Learning Goals 
Reflecting on the completion of the IST707 Fall 2023 project, it is clear how this project has solidified my understanding of data science principles and their application to complex real-world problems. The process of understanding, visualizing, and preprocessing a large dataset to increase the efficiency and accuracy in various classification algorithms was a huge endeavor to enhance my skill sets as a data scientist. It not only tested my technical skills but also pushed me to think critically about what entails being a data scientist, that is, 99% of the work involves data cleaning and preparation. 
The challenge of addressing data imbalance through techniques like SMOTE was particularly illuminating. This part of the project was a stark reminder of the crucial role that data preparation plays in the broader context of data science, reinforcing that accurate analysis stems from diligently curated data. 
